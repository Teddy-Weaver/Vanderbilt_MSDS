---
title: "Precision of a Sampling Distribution's Summary Statistics"
author: Teddy Weaver 
output:
  pdf_document:
    toc: TRUE
  html_document:
    theme: cosmo
  editor_options:
    chunk_output_type: inline
---

<script src="assets/code_folding.js"></script>
<script src="assets/code_folding.css"></script>

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(ggplot2)
library(gridExtra)
library(knitr)
library(RColorBrewer)
```

```{r helper_functions, include=FALSE}
`%|%` <- function(a,b) paste0(a,b)
```

[Random sampling](https://en.wikipedia.org/wiki/Sampling_(statistics)) is used to **estimate** summary statistics of a larger population as measuring the entire population is often impracticle or downright impossible. This process creates a probability distribution of the sampling statistics, also called the sampling distribution. 

A side-effect of random sampling is that it causes uncertainty -- this is why it results in a distribution and not a single value. This blog post dives into this uncertainty to show that certain quantiles can be estimated more accurately than others and how this depends to the distribution.

## **The Setup**
Here are the included parameters and brief description. Expand.grid is being used to create a matrix of all combinations. Each row can then be passed as a set of parameters, allowing us to iterate through each combination of settings.
```{r settings}
p_seq <- seq(.05,.95, .05) # Probability intervals

sim_settings <- expand.grid(
  N = 200, # Sample Size
  M = 5000, # Number of Samples
  D = c("norm", "exp", "f3", "f4"), # Distributions
  KEEP.OUT.ATTRS = FALSE, 
  stringsAsFactors = FALSE
)
```

```{r setup_results, include=FALSE}
results <- vector("list", nrow(sim_settings))
```
<br>
The settings also include an array of the four distributions used as examples. Both f3 and f4 are mixed distributions that can described by the their respective density functions below.

```{r mixed_distributions}
df3 <- function(x){
  .5*dnorm(x) + .3*dnorm(x,4) + .2*dnorm(x,-4,2)
}

df4 <- function(x) {
  .5*dbeta(x,5,1) + .5*dbeta(x,1,5)
}
```

```{r mixed_distributions2, include=FALSE}
# Mixed distribution #1
rf3 <- function(N){
  G <- sample(0:2, N, replace = TRUE, prob = c(5,3,2))
  (G==0)*rnorm(N) + (G==1)*rnorm(N,4) + (G==2)*rnorm(N,-4,2)
}

pf3 <- function(x){
  .5*pnorm(x) + .3*pnorm(x,4) + .2*pnorm(x,-4,2)
}

df3 <- function(x){
  .5*dnorm(x) + .3*dnorm(x,4) + .2*dnorm(x,-4,2)
}


g <- function(x,pf,p) {
  return(pf(x) - p)
}

qf3 <- function(p){
  out <- p
  for(i in seq_along(p)){
    out[i] <- uniroot(g, c(-100,100), pf = pf3, p = p[i])$root
  }
  out
}

# Mixed distribution #2
rf4 <- function(N){
  G <- sample(0:1, N, replace = TRUE)
  (G==0)*rbeta(N,5,1) + (G==1)*rbeta(N,1,5)
}

pf4 <- function(x) {
  .5*pbeta(x,5,1) + .5*pbeta(x,1,5)
}

qf4 <- function(p){
  out <- p
  for(i in seq_along(p)){
    out[i] <- uniroot(g, c(0,1), pf = pf4, p = p[i])$root
  }
  return (out)
}
```

## **The How**
To visualize uncertainty, we'll be finding the sampling distribution of the length of the middle 95% -- or the difference between the 2.5% and 97.5% percentiles. We can think of this summary statistic as the width of the distribution, excluding outliers in a hypothetical data set.

We'll also be taking a porgrammatic approach to solving this problem. As a result, some helper functions have been created to return the various required distribution functions.

```{r fun_get_distribution}
get_dist <- function(D) {
  return(
    list("rand" = get("r" %|% D),
         "quant" = get("q" %|% D),
         "dens" = get("d" %|% D),
         "dist" = get("p" %|% D))
    )
}
```

<br>

To find the mid-95% length for each of the 19 probability values in ```p_seq``` we used random distribution functions to create 5,000 samples of size N (`r sim_settings$N[1]`). The length was then found by subtracting the .025 quantile from the .975 quantile.

In addition, quantile and density values were found for each probability value using their respective distribution functions. This allows us to easily compare the mid-95% length to the density and cumulative distribution functions.

<div class="fold s">
```{r fun_calc_length}
calc_length <- function(D, N, M, p) {
  len_p <- length(p)
  
  sim_quantile <- array(NA, dim = c(M,len_p))
  sim_length <- vector(length=len_p)

  set.seed(1)
  for(i in 1:M){
    sim_quantile[i,] <- D[[1]](N) %>% quantile(probs = p)
  }
  
  for(i in 1:len_p) {
    quant <- quantile(sim_quantile[,i], c(.025, 0.975))
    sim_length[i] <- diff(quant)
  }
  return(sim_length)
}

# Derive Length and Density of the function. Output into dataframe.
main <- function(params, p_seq) {
  inputs <- as.list(params)
  
  with(inputs, {
    dist.type <- get_dist(D)
    s.length <- calc_length(dist.type["rand"], N, M, p_seq)
    s.quantile <- p_seq %>% (dist.type["quant"][[1]])
    s.density <- s.quantile %>% (dist.type["dens"][[1]])
    return(
      data.frame("prob" = p_seq,
                 "dist" = D,
                 "N" = N,
                 "M" = M,
                 "length" = s.length,
                 "density" = s.density,
                 "quantile" = s.quantile
                 )
      )
  })
}
```
</div>

<br>
This process is done for each of our settings combinations in the ```sim_settings``` data frame. The results data frame from each iteration is added to a list then combined into a final data frame using ```rbindlist```. Below is a sample of the combined results dataframe.

```{r fun_main, echo=FALSE}

for (i in 1:nrow(sim_settings)) {
  results[[i]] <- main(sim_settings[i,], p_seq)
}
results <- rbindlist(results)
knitr::kable(results[1:5,])
```
<br>

```{r fun_grapher, include=FALSE}
dist_grapher <- function(data, D, x_axis, y_axis, title, median = TRUE) {
  data_subset <- subset(data, dist == D)
  
  min_length <- (data_subset %>% 
  filter(length == min(length)))
  
  x.min_length <- (min_length %>% select(x_axis))[[1]]
  y.min_length <- (min_length%>% select(y_axis))[[1]]
  
  p <- ggplot(data_subset) +
    geom_line(aes_string(x=x_axis, y=y_axis)) +
    annotate(
        geom = "point",
        x = x.min_length,
        y = y.min_length,
        size = 4,
        colour = "#409852") +
    theme_minimal() +
    labs(x = tools::toTitleCase(x_axis),
         y = tools::toTitleCase(y_axis),
         title = tools::toTitleCase(title)) +
  theme(
    plot.title = element_text(size=18, color="#404040"),
    axis.title = element_text(size=14, color="#666666"),
    axis.text = element_text(size=10, color="#666666"),
    panel.grid.minor = element_blank())
  
    x.median <- data_subset %>%
      select(x_axis)
    x.median <- median(x.median[[1]])

    if(x_axis == "quantile") {
      if(y_axis == "density") {
        y.median <- get("d" %|% D)(x.median)
      } else {
        y.median <- (data_subset %>% filter(quantile == x.median) %>% select(y_axis))[[1]]
      }
    } else if (x_axis == "prob") {
      y.median <- (data_subset %>% filter(prob == 0.5) %>% select(y_axis))[[1]]
    } else if (x_axis == "density") {
      y.median <- (data_subset %>%
                     filter(round(density,7) == round(x.median,7)) %>%
                     select(y_axis))[[1]][1]
    }
    
    p <- p + 
      annotate(
        geom = "point",
        x = x.median,
        y = y.median,
        size = 4,
        colour = "#535056")
    
    
    return(p)
}
```

## **The Results**
As one might expect the shortest length, or location of most greatest accuracy, occurs during the densest region of the distribution - also the steepest slope on the CDF. Below are various graphs for each distribution to illustrate how the quantiles with more precision (shortest length) compare to the median, with both called out on each graph.

* **<span style="color:#409852">Green Dot</span>**: Shortest Length 
* **<span style="color:#535056">Gray Dot</span>**: Median

#### **Normal Distribution**
```{r graphs_normal, fig.height = 5, fig.width = 16, echo=FALSE}
n.density <- dist_grapher(results, "norm", "quantile", "density", "Density Function")

n.cdf <- dist_grapher(results, "norm", "quantile", "prob", "Continuous Distribution") +
  scale_y_continuous(limits=c(0,1),
                     breaks=seq(0,1,.25))

n.length <- dist_grapher(results, "norm", "prob", "length", "Middle 95% Length") +
  scale_x_continuous(limits = c(0,1))

n.den_vs_len <- dist_grapher(results, "norm", "density", "length", "Density vs Length") +
  scale_y_continuous(limits=c(.3,.6))

grid.arrange(n.length, n.den_vs_len, n.density, n.cdf, nrow=1)
```
As expected with the normal distributon, the median and shortest point occur very close together. Theoretically, they occur at the exact same point; however, because the random normal distribution was used we should expect a small amount of variance.

#### **Exponential Distribution**
```{r graphs_exponential, fig.height = 5, fig.width = 16, echo=FALSE}
exp.density <- dist_grapher(results, "exp", "quantile", "density", "Density Function")

exp.cdf <- dist_grapher(results, "exp", "quantile", "prob", "Continuous Distribution") +
  scale_y_continuous(limits=c(0,1),
                     breaks=seq(0,1,.25))

exp.length <- dist_grapher(results, "exp", "prob", "length", "Middle 95% Length")

exp.den_vs_len <- dist_grapher(results, "exp", "density", "length", "Density vs Length") 

grid.arrange(exp.length, exp.den_vs_len,exp.density, exp.cdf, nrow=1)
```
In an exponential distribution values are clustered at the start, which we see with the shortest length occuring at the first value.

#### **Mixed Distribution (F3)**
```{r graphs_f3, fig.height = 5, fig.width = 16, echo=FALSE}
f3.density <- dist_grapher(results, "f3", "quantile", "density", "Density Function")

f3.cdf <- dist_grapher(results, "f3", "quantile", "prob", "Continuous Distribution")

f3.length <- dist_grapher(results, "f3", "prob", "length", "Middle 95% Length")

f3.den_vs_len <- dist_grapher(results, "f3", "density", "length", "Density vs Length") 

grid.arrange(f3.length, f3.den_vs_len, f3.density, f3.cdf, nrow=1)
```
Mixed distribution f3 happens to be similar to the normal, in terms of median and shortest point. This is a great example of why it is important to know the distribution

#### **Mixed Distribution (F4)**
```{r graphs_f4, fig.height = 5, fig.width = 16, echo=FALSE}
f4.density <- dist_grapher(results, "f4", "quantile", "density", "Density Function") +
  annotate(geom = "point", x = 0.02084282, y = 2.2979986, size = 4, colour = "#409852")

f4.cdf <- dist_grapher(results, "f4", "quantile", "prob", "Continuous Distribution") +
  annotate(geom = "point", x = 0.02084282, y = 0.05, size = 4, colour = "#409852")

f4.length <- dist_grapher(results, "f4", "prob", "length", "Middle 95% Length") +
  annotate(geom = "point", x = 0.05, y = 0.02704438, size = 4, colour = "#409852")

f4.den_vs_len <- dist_grapher(results, "f4", "density", "length", "Density vs Length")

grid.arrange(f4.length, f4.den_vs_len,f4.density, f4.cdf, nrow=1)
```
Mixed distriubiton f4 is very close to the opposite of the normal distribution. We can observe that both ends of the distribution (Density Function) are equally dense, resulting in two areas of high relative precision.

## **The Wrap Up**
Hopefully what this illustrated is the importance of correctly identifying or modeling the distribution of a data set. Without it, one cannot quantify the precision of summmary statistics from sample distributions.

With that said, one method of improving precision, is to increase the sample size. Below is a figure that illustrates this for an exponential distribution. As the sample size doubles, the precision is increased a substantial amount. It is important to also notice the diminishing returns as the sample size increases. When using sample distributions, one must strike a balance between precision of estimated summary statistics and avoiding oversampling.

```{r echo=FALSE}
sim_settings2 <- expand.grid(
  N = c(200, 400, 800, 1600), # Sample Size
  M = 5000, # Number of Samples
  D =  "exp", # Distributions
  KEEP.OUT.ATTRS = FALSE, 
  stringsAsFactors = FALSE
)

results2 <- vector("list", nrow(sim_settings2))

for (i in 1:nrow(sim_settings2)) {
  results2[[i]] <- main(sim_settings2[i,], p_seq)
}

results2 <- rbindlist(results2)
results2$N <- as.factor(results2$N)

ggplot(results2, aes(x=density, y=length, group=N)) +
  geom_line(aes(color=N)) +
  geom_point(aes(color=N)) +
  scale_color_brewer(palette = "Paired") +
  scale_x_continuous(limits=c(0,1)) +
  labs(x="Density",
       y="Length") +
  theme_bw() +
  theme(legend.position="bottom")
```



